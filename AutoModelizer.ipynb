{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoModelizer\n",
    "---\n",
    "\n",
    "La idea de este proyecto es encontrar el mejor modelo de CNN que se adapte al dataset correspondiente, para ello usando algoritmos evolutivos. Este tipo de soluciones se conocen como neuroevoluciones\n",
    "\n",
    "A continuación un ejemplo básico de como funcionan este tipo de algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def fitness(x):\n",
    "    return x ** 2\n",
    "\n",
    "population_size = 10\n",
    "population = np.random.uniform(-10, 10, population_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_parent_tournament(population, scores, k=3):\n",
    "    selection_ix = np.random.randint(len(population), size=k)\n",
    "    selected = population[selection_ix]\n",
    "    ix = np.argmax(scores[selection_ix])\n",
    "    return selected[ix]\n",
    "\n",
    "def crossover_test(p1, p2):\n",
    "    child = (p1 + p2) / 2\n",
    "    return child\n",
    "\n",
    "def mutate(x):\n",
    "    mutation_chance = 0.1\n",
    "    if np.random.rand() < mutation_chance:\n",
    "        x += np.random.uniform(-1, 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generación 0, x = 7.235685966251891 Mejor puntuación 95.5815681029002\n",
      "Generación 1, x = 9.391867825099322 Mejor puntuación 99.41869535871274\n",
      "Generación 2, x = 9.238513354052039 Mejor puntuación 99.41869535871274\n",
      "Generación 3, x = 9.435658013878848 Mejor puntuación 99.41869535871274\n",
      "Generación 4, x = 9.946603684577202 Mejor puntuación 98.93492485804477\n",
      "Generación 5, x = 10.379909813023712 Mejor puntuación 116.9256389968657\n",
      "Generación 6, x = 10.596562877246967 Mejor puntuación 116.9256389968657\n",
      "Generación 7, x = 10.928618281175508 Mejor puntuación 126.8027718423948\n",
      "Generación 8, x = 11.260673685104049 Mejor puntuación 126.8027718423948\n",
      "Generación 9, x = 11.205377196142319 Mejor puntuación 136.1009992530539\n",
      "Generación 10, x = 11.403888377133416 Mejor puntuación 133.33558928986858\n",
      "Generación 11, x = 11.839113582440305 Mejor puntuación 146.1731577869418\n",
      "Generación 12, x = 11.874762490023356 Mejor puntuación 142.84507517211782\n",
      "Generación 13, x = 11.923614337009063 Mejor puntuación 142.84507517211782\n",
      "Generación 14, x = 12.577805455720549 Mejor puntuación 142.84507517211782\n",
      "Generación 15, x = 12.264793355459599 Mejor puntuación 158.2011900819536\n",
      "Generación 16, x = 11.577435626468084 Mejor puntuación 163.25361278753326\n",
      "Generación 17, x = 12.659148883982379 Mejor puntuación 172.24624777894164\n",
      "Generación 18, x = 13.919958155323526 Mejor puntuación 193.76523504595795\n",
      "Generación 19, x = 13.289553519652952 Mejor puntuación 193.76523504595795\n",
      "Generación 20, x = 13.400667430275416 Mejor puntuación 193.76523504595795\n",
      "Generación 21, x = 13.67314505149084 Mejor puntuación 189.4024700965219\n",
      "Generación 22, x = 13.68355641694706 Mejor puntuación 189.4024700965219\n",
      "Generación 23, x = 13.971807845110435 Mejor puntuación 201.74116439880723\n",
      "Generación 24, x = 13.844779434529396 Mejor puntuación 195.2114144606895\n",
      "Generación 25, x = 13.966232098553245 Mejor puntuación 195.2114144606895\n",
      "Generación 26, x = 13.969019971831841 Mejor puntuación 195.2114144606895\n",
      "Generación 27, x = 13.971110876790785 Mejor puntuación 195.2114144606895\n",
      "Generación 28, x = 13.44072875030427 Mejor puntuación 217.40748432135976\n",
      "Generación 29, x = 13.971110876790785 Mejor puntuación 208.85160217554477\n",
      "Generación 30, x = 14.404813261587918 Mejor puntuación 207.49864510121915\n",
      "Generación 31, x = 14.404813261587918 Mejor puntuación 207.49864510121915\n",
      "Generación 32, x = 14.404813261587918 Mejor puntuación 207.49864510121915\n",
      "Generación 33, x = 14.404813261587918 Mejor puntuación 207.49864510121915\n",
      "Generación 34, x = 14.404807816522922 Mejor puntuación 207.49864510121915\n",
      "Generación 35, x = 14.404813261587918 Mejor puntuación 207.49864510121915\n",
      "Generación 36, x = 14.778793309493736 Mejor puntuación 226.20025844887343\n",
      "Generación 37, x = 14.722384331041372 Mejor puntuación 218.4127316847368\n",
      "Generación 38, x = 14.764691064880644 Mejor puntuación 218.4127316847368\n",
      "Generación 39, x = 14.764691064880644 Mejor puntuación 218.4127316847368\n",
      "Generación 40, x = 14.775267748340463 Mejor puntuación 240.80500654676203\n",
      "Generación 41, x = 15.144817648164679 Mejor puntuación 229.47230198728937\n",
      "Generación 42, x = 15.148343209317954 Mejor puntuación 229.47230198728937\n",
      "Generación 43, x = 15.147021123885477 Mejor puntuación 229.47230198728937\n",
      "Generación 44, x = 15.147682166601715 Mejor puntuación 229.45895057891275\n",
      "Generación 45, x = 15.147461819029635 Mejor puntuación 229.45895057891275\n",
      "Generación 46, x = 15.147627079708695 Mejor puntuación 256.1070009657354\n",
      "Generación 47, x = 15.147682166601715 Mejor puntuación 256.1070009657354\n",
      "Generación 48, x = 15.468555140694885 Mejor puntuación 249.3060401920596\n",
      "Generación 49, x = 15.682470456757002 Mejor puntuación 245.93987962705617\n",
      "Generación 50, x = 15.682470456757002 Mejor puntuación 255.29655369569315\n",
      "Generación 51, x = 15.403076371287266 Mejor puntuación 255.29655369569315\n",
      "Generación 52, x = 16.306503245015524 Mejor puntuación 272.7132370333876\n",
      "Generación 53, x = 16.361553452251925 Mejor puntuación 272.7132370333876\n",
      "Generación 54, x = 16.792533210488415 Mejor puntuación 291.42023260087797\n",
      "Generación 55, x = 16.65328236509031 Mejor puntuación 291.42023260087797\n",
      "Generación 56, x = 16.845407716352998 Mejor puntuación 291.42023260087797\n",
      "Generación 57, x = 16.958221308818814 Mejor puntuación 291.42023260087797\n",
      "Generación 58, x = 17.036222189935103 Mejor puntuación 291.42023260087797\n",
      "Generación 59, x = 17.016721969656032 Mejor puntuación 290.2328665048372\n",
      "Generación 60, x = 17.036222189935103 Mejor puntuación 308.0493848925007\n",
      "Generación 61, x = 17.165000567724853 Mejor puntuación 299.0747902163243\n",
      "Generación 62, x = 17.293778945514607 Mejor puntuación 327.0510286067767\n",
      "Generación 63, x = 17.293778945514607 Mejor puntuación 327.0510286067767\n",
      "Generación 64, x = 17.689165577350337 Mejor puntuación 327.0510286067767\n",
      "Generación 65, x = 17.689165577350337 Mejor puntuación 319.93972106768786\n",
      "Generación 66, x = 17.886858893268204 Mejor puntuación 319.93972106768786\n",
      "Generación 67, x = 17.973294111970105 Mejor puntuación 355.8880419454986\n",
      "Generación 68, x = 18.217828175728823 Mejor puntuación 340.85881946205535\n",
      "Generación 69, x = 18.41914463013659 Mejor puntuación 339.26488890588956\n",
      "Generación 70, x = 18.7284765929871 Mejor puntuación 362.4381546087239\n",
      "Generación 71, x = 18.7284765929871 Mejor puntuación 350.7558354940657\n",
      "Generación 72, x = 18.73150750530759 Mejor puntuación 350.9829297215826\n",
      "Generación 73, x = 18.73150750530759 Mejor puntuación 350.8693734213946\n",
      "Generación 74, x = 19.135930124203313 Mejor puntuación 366.4879934061283\n",
      "Generación 75, x = 19.03681097174699 Mejor puntuación 366.4879934061283\n",
      "Generación 76, x = 18.228556905523558 Mejor puntuación 362.94841534261144\n",
      "Generación 77, x = 19.046613308542547 Mejor puntuación 362.87272876458275\n",
      "Generación 78, x = 19.046613308542547 Mejor puntuación 362.77347852515004\n",
      "Generación 79, x = 19.046613308542547 Mejor puntuación 362.77347852515004\n",
      "Generación 80, x = 19.22594402641834 Mejor puntuación 362.77347852515004\n",
      "Generación 81, x = 19.144171300861768 Mejor puntuación 375.9987015231841\n",
      "Generación 82, x = 19.780816744181912 Mejor puntuación 410.19691031484376\n",
      "Generación 83, x = 20.575486375595265 Mejor puntuación 407.52888971551897\n",
      "Generación 84, x = 19.95109391256312 Mejor puntuación 423.3506395923064\n",
      "Generación 85, x = 20.42763497360906 Mejor puntuación 423.3506395923064\n",
      "Generación 86, x = 20.575486375595265 Mejor puntuación 423.3506395923064\n",
      "Generación 87, x = 20.93680228365152 Mejor puntuación 453.60983850795367\n",
      "Generación 88, x = 20.93680228365152 Mejor puntuación 438.3496898647155\n",
      "Generación 89, x = 20.93680228365152 Mejor puntuación 438.3496898647155\n",
      "Generación 90, x = 20.93680228365152 Mejor puntuación 438.3496898647155\n",
      "Generación 91, x = 20.93680228365152 Mejor puntuación 438.3496898647155\n",
      "Generación 92, x = 20.93680228365152 Mejor puntuación 438.3496898647155\n",
      "Generación 93, x = 21.17263922954569 Mejor puntuación 458.32285215437616\n",
      "Generación 94, x = 21.503817385355674 Mejor puntuación 476.76703028272414\n",
      "Generación 95, x = 21.503817385355674 Mejor puntuación 462.41416214272493\n",
      "Generación 96, x = 21.503817385355674 Mejor puntuación 462.41416214272493\n",
      "Generación 97, x = 21.503817385355674 Mejor puntuación 462.41416214272493\n",
      "Generación 98, x = 21.503817385355674 Mejor puntuación 462.41416214272493\n",
      "Generación 99, x = 21.628391150113863 Mejor puntuación 493.57831481523795\n",
      "Generación 100, x = 21.984793701896614 Mejor puntuación 483.331154114953\n",
      "Generación 101, x = 21.082866343731666 Mejor puntuación 510.63391189246295\n",
      "Generación 102, x = 22.259858543717364 Mejor puntuación 496.8887694911239\n",
      "Generación 103, x = 22.236402611874773 Mejor puntuación 496.8887694911239\n",
      "Generación 104, x = 22.27932319446083 Mejor puntuación 496.5417208560579\n",
      "Generación 105, x = 22.27932319446083 Mejor puntuación 496.3682420032403\n",
      "Generación 106, x = 22.278349961923656 Mejor puntuación 496.3682420032403\n",
      "Generación 107, x = 23.148718161615623 Mejor puntuación 496.3682420032403\n",
      "Generación 108, x = 23.148718161615623 Mejor puntuación 535.8631525259129\n",
      "Generación 109, x = 23.431521099711013 Mejor puntuación 562.3691645700827\n",
      "Generación 110, x = 22.430274877547465 Mejor puntuación 562.3691645700827\n",
      "Generación 111, x = 23.86185098695097 Mejor puntuación 562.3691645700827\n",
      "Generación 112, x = 22.98386786801296 Mejor puntuación 599.0506660273164\n",
      "Generación 113, x = 24.16868127249106 Mejor puntuación 584.1251544512601\n",
      "Generación 114, x = 24.073532832462966 Mejor puntuación 584.1251544512601\n",
      "Generación 115, x = 24.16868127249106 Mejor puntuación 616.2615759165745\n",
      "Generación 116, x = 24.332665036792122 Mejor puntuación 626.3876991383685\n",
      "Generación 117, x = 24.71141313151422 Mejor puntuación 613.1662367191552\n",
      "Generación 118, x = 24.69580747349895 Mejor puntuación 613.1662367191552\n",
      "Generación 119, x = 24.73680341457421 Mejor puntuación 611.9094431712904\n",
      "Generación 120, x = 24.730455843809214 Mejor puntuación 653.1057029115859\n",
      "Generación 121, x = 24.93841198353784 Mejor puntuación 632.339829790832\n",
      "Generación 122, x = 25.042390053402155 Mejor puntuación 632.1802215126005\n",
      "Generación 123, x = 25.14319433788397 Mejor puntuación 632.1802215126005\n",
      "Generación 124, x = 25.14319433788397 Mejor puntuación 632.1802215126005\n",
      "Generación 125, x = 25.14319433788397 Mejor puntuación 632.1802215126005\n",
      "Generación 126, x = 25.14319433788397 Mejor puntuación 632.1802215126005\n",
      "Generación 127, x = 25.14319433788397 Mejor puntuación 632.1802215126005\n",
      "Generación 128, x = 25.14319433788397 Mejor puntuación 632.1802215126005\n",
      "Generación 129, x = 25.14319433788397 Mejor puntuación 673.6597545684962\n",
      "Generación 130, x = 25.54907530000135 Mejor puntuación 673.6597545684962\n",
      "Generación 131, x = 25.33922961522046 Mejor puntuación 673.6597545684962\n",
      "Generación 132, x = 26.326302377113596 Mejor puntuación 675.6515625854178\n",
      "Generación 133, x = 26.039159079086815 Mejor puntuación 693.0741968512169\n",
      "Generación 134, x = 26.12511003426656 Mejor puntuación 692.5972428379637\n",
      "Generación 135, x = 26.317242310659445 Mejor puntuación 692.5972428379637\n",
      "Generación 136, x = 26.317242310659445 Mejor puntuación 701.6984697285151\n",
      "Generación 137, x = 26.359763409955896 Mejor puntuación 701.6984697285151\n",
      "Generación 138, x = 26.938147389796057 Mejor puntuación 732.832196354739\n",
      "Generación 139, x = 26.837646344151114 Mejor puntuación 742.0322939868933\n",
      "Generación 140, x = 27.26065846122917 Mejor puntuación 744.2555368948614\n",
      "Generación 141, x = 27.176746162345307 Mejor puntuación 743.1434997397855\n",
      "Generación 142, x = 27.231188557797182 Mejor puntuación 741.9894855708862\n",
      "Generación 143, x = 27.239483944650754 Mejor puntuación 741.9894855708862\n",
      "Generación 144, x = 27.239483944650754 Mejor puntuación 741.9894855708862\n",
      "Generación 145, x = 27.239483944650754 Mejor puntuación 780.1031825029096\n",
      "Generación 146, x = 27.473665050497388 Mejor puntuación 760.9270179050905\n",
      "Generación 147, x = 28.251965048947444 Mejor puntuación 798.173529126948\n",
      "Generación 148, x = 27.96160237384858 Mejor puntuación 798.173529126948\n",
      "Generación 149, x = 27.730009413259744 Mejor puntuación 798.173529126948\n",
      "Generación 150, x = 28.251965048947444 Mejor puntuación 798.173529126948\n",
      "Generación 151, x = 28.251965048947444 Mejor puntuación 798.173529126948\n",
      "Generación 152, x = 28.251965048947444 Mejor puntuación 798.173529126948\n",
      "Generación 153, x = 28.251965048947444 Mejor puntuación 810.7804728205402\n",
      "Generación 154, x = 28.251965048947444 Mejor puntuación 804.4646530858752\n",
      "Generación 155, x = 28.30752557420808 Mejor puntuación 804.4646530858752\n",
      "Generación 156, x = 28.88564147268866 Mejor puntuación 834.3802832887112\n",
      "Generación 157, x = 28.88564147268866 Mejor puntuación 842.9739750330563\n",
      "Generación 158, x = 28.88564147268866 Mejor puntuación 838.6716255513996\n",
      "Generación 159, x = 28.941281209700257 Mejor puntuación 838.6716255513996\n",
      "Generación 160, x = 28.922734630696393 Mejor puntuación 838.6716255513996\n",
      "Generación 161, x = 29.380454592595605 Mejor puntuación 838.1346058112762\n",
      "Generación 162, x = 28.945917854451224 Mejor puntuación 863.2111120675721\n",
      "Generación 163, x = 29.164345384711154 Mejor puntuación 850.6266554168493\n",
      "Generación 164, x = 29.165504545898898 Mejor puntuación 853.9684958562533\n",
      "Generación 165, x = 29.194121954264865 Mejor puntuación 856.6483494052153\n",
      "Generación 166, x = 29.249947220532142 Mejor puntuación 856.6483494052153\n",
      "Generación 167, x = 29.240643009487595 Mejor puntuación 855.559412403916\n",
      "Generación 168, x = 29.70330917146862 Mejor puntuación 909.4248131849017\n",
      "Generación 169, x = 29.70330917146862 Mejor puntuación 909.4248131849017\n",
      "Generación 170, x = 30.53647345182541 Mejor puntuación 909.4248131849017\n",
      "Generación 171, x = 30.259278440796173 Mejor puntuación 932.4762108740382\n",
      "Generación 172, x = 30.318237165181728 Mejor puntuación 932.4762108740382\n",
      "Generación 173, x = 30.42735530850357 Mejor puntuación 932.4762108740382\n",
      "Generación 174, x = 31.391889833514835 Mejor puntuación 932.4762108740382\n",
      "Generación 175, x = 30.946856954480793 Mejor puntuación 985.4507473195321\n",
      "Generación 176, x = 30.946856954480793 Mejor puntuación 957.7079553610962\n",
      "Generación 177, x = 30.946856954480793 Mejor puntuación 957.7079553610962\n",
      "Generación 178, x = 30.946856954480793 Mejor puntuación 964.9753645929447\n",
      "Generación 179, x = 31.26213748400559 Mejor puntuación 987.4386258869771\n",
      "Generación 180, x = 31.342836830659916 Mejor puntuación 987.4386258869771\n",
      "Generación 181, x = 31.214495479922572 Mejor puntuación 987.4386258869771\n",
      "Generación 182, x = 31.38318650398708 Mejor puntuación 984.9043951440368\n",
      "Generación 183, x = 31.32083891130737 Mejor puntuación 984.9043951440368\n",
      "Generación 184, x = 31.342502560598547 Mejor puntuación 984.9043951440368\n",
      "Generación 185, x = 31.366855199503355 Mejor puntuación 984.9043951440368\n",
      "Generación 186, x = 31.377398388507387 Mejor puntuación 984.9043951440368\n",
      "Generación 187, x = 31.37884541737731 Mejor puntuación 988.1330594146394\n",
      "Generación 188, x = 31.407438128972405 Mejor puntuación 988.1330594146394\n",
      "Generación 189, x = 31.40028995107363 Mejor puntuación 1009.4911324584867\n",
      "Generación 190, x = 31.589964156219544 Mejor puntuación 1009.4911324584867\n",
      "Generación 191, x = 31.589964156219544 Mejor puntuación 1061.151184719898\n",
      "Generación 192, x = 32.35879449777327 Mejor puntuación 1041.50904316272\n",
      "Generación 193, x = 32.19669514808545 Mejor puntuación 1047.0915813491215\n",
      "Generación 194, x = 32.75684909869058 Mejor puntuación 1099.2476393302727\n",
      "Generación 195, x = 32.74605219061757 Mejor puntuación 1073.011162874386\n",
      "Generación 196, x = 32.71010476591529 Mejor puntuación 1073.011162874386\n",
      "Generación 197, x = 33.161905217105144 Mejor puntuación 1126.7408933002755\n",
      "Generación 198, x = 32.95937715789786 Mejor puntuación 1126.7408933002755\n",
      "Generación 199, x = 33.363083662803305 Mejor puntuación 1174.6456853865384\n",
      "Mejor solución: x = 33.71750480981623, f(x) = 1136.870130599981\n"
     ]
    }
   ],
   "source": [
    "n_generations = 200\n",
    "\n",
    "for generation in range(n_generations):\n",
    "    scores = np.array([fitness(x) for x in population])\n",
    "    new_population = []\n",
    "    for _ in range(population_size):\n",
    "        parent1 = select_parent_tournament(population, scores)\n",
    "        parent2 = select_parent_tournament(population, scores)\n",
    "        child = crossover_test(parent1, parent2)\n",
    "        child = mutate(child)\n",
    "        new_population.append(child)\n",
    "    population = np.array(new_population)\n",
    "    best_score = np.max(scores)\n",
    "    print(f\"Generación {generation}, x = {child} Mejor puntuación {best_score}\")\n",
    "\n",
    "best_solution = population[np.argmax(scores)]\n",
    "print(f\"Mejor solución: x = {best_solution}, f(x) = {fitness(best_solution)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos el dataset de prueba (MNIST)\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## indicamos la máxima descendencia que queremos \n",
    "# uso 2 para simplemente comprobar que el algoritmo al completo funciona, este valor lo \n",
    "# indicará el usuario desde la interfaz\n",
    "max_desc = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS DEL DATASET DE PRUEBA\n",
    "\n",
    "num_channels = 1 # escala de grises\n",
    "px_h = 28\n",
    "px_w = 28\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preparar los DataLoader para los conjuntos de entrenamiento y validación\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((px_h, px_w)),\n",
    "    transforms.Grayscale(num_output_channels=num_channels),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "val_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos el dataset de prueba (CIFAR100)\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DATOS DEL DATASET DE PRUEBA CIFAR 100\n",
    "num_channels = 3 # escala de colores\n",
    "px_h = 32\n",
    "px_w = 32\n",
    "batch_size = 128\n",
    "num_classes = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preparar los DataLoader para los conjuntos de entrenamiento y validación\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((px_h, px_w)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "val_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERACIÓN DE LA RED EN BASE A VECTORES\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comprobamos si tenemos acceso a la GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU) está disponible en tu sistema.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA (GPU) está disponible en tu sistema.\")\n",
    "else:\n",
    "    print(\"CUDA (GPU) no está disponible en tu sistema.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construimos una función que sea capaz de crear modelos en base a vectores que representen la arquitectura de la red.\n",
    "De este modo el algorimo evolutivo puede ir adaptando y cambiando la red fácilmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "filter_range = [8,16,32,64,128]\n",
    "kernel_size_range = [1,3,5,7,11] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def build_cnn_from_individual(individual):\n",
    "    \"\"\" \n",
    "    Funcion para construir un modelo en base a un diccionario\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    num_layers = individual['num_conv_layers']\n",
    "    fully_connected = int(individual['fully_connected'])\n",
    "    dropout = individual['dropout']\n",
    "    out_channels_previous_layer = num_channels\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        out_channels = individual['filters'][i]\n",
    "        kernel_size = individual['kernel_sizes'][i]\n",
    "        \n",
    "        conv_layer = nn.Conv2d(out_channels_previous_layer, out_channels, kernel_size=kernel_size, padding=1)\n",
    "        layers.append(conv_layer)\n",
    "        if out_channels_previous_layer > 1 or i > 0:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU())\n",
    "        if i < 1:\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        else:\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2, stride=1))\n",
    "\n",
    "\n",
    "        out_channels_previous_layer = out_channels\n",
    "\n",
    "\n",
    "    # Temporalmente crear un modelo para calcular el tamaño de salida de las capas convolucionales\n",
    "    temp_model = nn.Sequential(*layers)\n",
    "\n",
    "    # Calcular el tamaño de salida usando un tensor dummy\n",
    "    dummy_input = torch.zeros(1, num_channels, px_h, px_w)\n",
    "    output_size = temp_model(dummy_input).view(-1).shape[0]\n",
    "\n",
    "    # Ahora, sabiendo el tamaño de salida, podemos definir las capas lineales correctamente\n",
    "    layers.append(nn.Flatten())\n",
    "\n",
    "    for i in range(fully_connected):\n",
    "        layers.append(nn.Linear(in_features=output_size, out_features=output_size))\n",
    "        if dropout > 0:\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            dropout-= 1\n",
    "\n",
    "    layers.append(nn.Linear(output_size, num_classes))\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función para generar diccionarios para la población inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_individual(min_conv_layers, max_conv_layers, min_filters, max_filters, kernel_sizes):\n",
    "    \"\"\" \n",
    "    Funcion para generar un diccionario que representa a una arquitectura\n",
    "    \"\"\"\n",
    "    individual = {\n",
    "        'num_conv_layers': random.randint(min_conv_layers, max_conv_layers),\n",
    "        'filters': [],\n",
    "        'kernel_sizes': [],\n",
    "        'learning_rate': random.choice(learning_rates),\n",
    "        'fully_connected': random.randint(0,2),\n",
    "        'dropout': random.randint(0,2)\n",
    "    }\n",
    "\n",
    "    for _ in range(individual['num_conv_layers']):\n",
    "        individual['filters'].append(random.choice(filter_range))\n",
    "        individual['kernel_sizes'].append(random.choice(kernel_size_range))\n",
    "    \n",
    "    # Agrega más parámetros según sea necesario, como capas completamente conectadas, etc.\n",
    "\n",
    "    return individual\n",
    "\n",
    "def initialize_population(pop_size, min_conv_layers, max_conv_layers, min_filters, max_filters, kernel_sizes):\n",
    "    return [generate_individual(min_conv_layers, max_conv_layers, min_filters, max_filters, kernel_sizes) for _ in range(pop_size)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETROS PARA POSIBLES ARQUITECTURAS DE RED\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AJUSTAR SEGÚN VAYA NECESITANDO Y TIEMPO \n",
    "\n",
    "population_size = 10\n",
    "min_conv_layers = 1\n",
    "max_conv_layers = 3\n",
    "min_filters = 16\n",
    "max_filters = 128\n",
    "kernel_sizes = [3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'num_conv_layers': 1,\n",
       "  'filters': [16],\n",
       "  'kernel_sizes': [3],\n",
       "  'learning_rate': 0.0001,\n",
       "  'fully_connected': 2,\n",
       "  'dropout': 2},\n",
       " {'num_conv_layers': 1,\n",
       "  'filters': [32],\n",
       "  'kernel_sizes': [1],\n",
       "  'learning_rate': 1e-06,\n",
       "  'fully_connected': 2,\n",
       "  'dropout': 1},\n",
       " {'num_conv_layers': 2,\n",
       "  'filters': [8, 16],\n",
       "  'kernel_sizes': [7, 1],\n",
       "  'learning_rate': 0.01,\n",
       "  'fully_connected': 2,\n",
       "  'dropout': 2},\n",
       " {'num_conv_layers': 2,\n",
       "  'filters': [64, 128],\n",
       "  'kernel_sizes': [5, 3],\n",
       "  'learning_rate': 0.01,\n",
       "  'fully_connected': 1,\n",
       "  'dropout': 2},\n",
       " {'num_conv_layers': 1,\n",
       "  'filters': [64],\n",
       "  'kernel_sizes': [3],\n",
       "  'learning_rate': 0.0001,\n",
       "  'fully_connected': 0,\n",
       "  'dropout': 1},\n",
       " {'num_conv_layers': 1,\n",
       "  'filters': [16],\n",
       "  'kernel_sizes': [5],\n",
       "  'learning_rate': 0.0001,\n",
       "  'fully_connected': 1,\n",
       "  'dropout': 1},\n",
       " {'num_conv_layers': 2,\n",
       "  'filters': [128, 64],\n",
       "  'kernel_sizes': [11, 1],\n",
       "  'learning_rate': 0.01,\n",
       "  'fully_connected': 2,\n",
       "  'dropout': 2},\n",
       " {'num_conv_layers': 1,\n",
       "  'filters': [16],\n",
       "  'kernel_sizes': [3],\n",
       "  'learning_rate': 1e-06,\n",
       "  'fully_connected': 2,\n",
       "  'dropout': 1},\n",
       " {'num_conv_layers': 2,\n",
       "  'filters': [64, 32],\n",
       "  'kernel_sizes': [5, 3],\n",
       "  'learning_rate': 0.01,\n",
       "  'fully_connected': 0,\n",
       "  'dropout': 2},\n",
       " {'num_conv_layers': 3,\n",
       "  'filters': [32, 64, 32],\n",
       "  'kernel_sizes': [1, 1, 1],\n",
       "  'learning_rate': 0.0001,\n",
       "  'fully_connected': 2,\n",
       "  'dropout': 1}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "population = initialize_population(population_size, min_conv_layers, max_conv_layers, min_filters, max_filters, kernel_sizes)\n",
    "\n",
    "epochs = 15 \n",
    "\n",
    "population\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENTRENAMIENTO Y EVALUACIÓN DE LOS MODELOS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "def evaluate_individual(individual, train_loader, val_loader, device='cuda', epochs=5):\n",
    "    \"\"\" \n",
    "    Funcion para entrenar y evaluar una arquitectura\n",
    "    \"\"\"\n",
    "    # Construir el modelo basado en el individuo\n",
    "    model = build_cnn_from_individual(individual).to(device)\n",
    "    \n",
    "    # Definir el optimizador y la función de pérdida\n",
    "    # HACER QUE EL OPTIMIZADOR SEA OTRO GEN!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=individual['learning_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Entrenamiento\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        progress_bar = tqdm(total=len(train_loader), desc=f'Epoch {epoch+1}/{epochs}', unit='batch')\n",
    "        for data, targets in train_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Actualizar la barra de progreso con la última información de pérdida\n",
    "            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item())})\n",
    "            progress_bar.update()  # Forzar la actualización de la barra de progreso\n",
    "            \n",
    "        progress_bar.close()  # Cerrar la barra de progreso al final de cada época\n",
    "\n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in val_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy  # Esta es la \"aptitud\" del individuo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUTACIONES Y CRUCES\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def mutate_individual(individual):\n",
    "    \"\"\"\n",
    "    Mutar un individuo cambiando aleatoriamente sus hiperparámetros.\n",
    "    \"\"\"\n",
    "    mutation_rate = 0.3  # Probabilidad de mutar cada característica\n",
    "    fully_connected_range = (0,5) #Rango para ver cuantas fully conected se añaden\n",
    "\n",
    "    if random.random() < mutation_rate:\n",
    "        # Mutar la tasa de aprendizaje\n",
    "        individual['learning_rate'] = random.choice(learning_rates)\n",
    "\n",
    "\n",
    "    # Asegurarse de que hay suficientes entradas en las listas 'filters' y 'kernel_sizes'\n",
    "    for i in range(individual['num_conv_layers']):\n",
    "        if random.random() < mutation_rate:\n",
    "            # Mutar el número de filtros en la capa i\n",
    "            individual['filters'][i] = random.choice(filter_range)\n",
    "        if random.random() < mutation_rate:\n",
    "            # Mutar el tamaño de filtro en la capa i\n",
    "            individual['kernel_sizes'][i] = random.choice(kernel_size_range)\n",
    "\n",
    "    if random.random() < mutation_rate:\n",
    "        # Mutar la tasa de aprendizaje\n",
    "        num_fully = random.randint(*fully_connected_range)\n",
    "        individual['fully_connected'] = num_fully\n",
    "        individual['dropout'] = random.randint(0, num_fully)\n",
    "    return individual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y el cruce entre individuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover(parent1, parent2):\n",
    "    \"\"\"\n",
    "    Realiza un cruce uniforme entre dos individuos.\n",
    "    \n",
    "    Args:\n",
    "        parent1 (dict): El primer individuo padre.\n",
    "        parent2 (dict): El segundo individuo padre.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Un nuevo individuo hijo.\n",
    "    \"\"\"\n",
    "    child = {}\n",
    "    for key in parent1:\n",
    "        if key == \"filters\" or key == \"kernel_sizes\":\n",
    "            if random.random() < 0.5:\n",
    "                child[\"filters\"] = parent1[\"filters\"]\n",
    "                child[\"kernel_sizes\"] = parent1[\"kernel_sizes\"]\n",
    "            else:\n",
    "                child[\"filters\"] = parent2[\"filters\"]\n",
    "                child[\"kernel_sizes\"] = parent2[\"kernel_sizes\"]\n",
    "\n",
    "        else:\n",
    "            if key != \"num_conv_layers\":\n",
    "                if random.random() < 0.5:\n",
    "                    child[key] = parent1[key]\n",
    "                else:\n",
    "                    child[key] = parent2[key]\n",
    "    child[\"num_conv_layers\"] = len(child[\"filters\"])\n",
    "\n",
    "    return child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos como evaluamos a la población "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_population(population, train_loader, val_loader, device, epochs):\n",
    "    \"\"\"\n",
    "    Funcion para evaluar a una poblacion de arquitecturas\n",
    "    \"\"\"\n",
    "    fitness_scores = []\n",
    "    \n",
    "    for individual in population:\n",
    "        print(individual)\n",
    "        fitness = evaluate_individual(individual, train_loader, val_loader, device, epochs)\n",
    "        fitness_scores.append(fitness)\n",
    "    \n",
    "    # sacamos los scores de la poblacion\n",
    "    return fitness_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recopilamos lo que hemos realizado, hemos creado las posibles mutaciones sobre las arquitecturas, los posibles cruces, la evaluación de los modelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queda realizar:\n",
    "- Selección de reproducción: por torneo en principio para también  puede ser por torneo o ruleta\n",
    "- Creación de la nueva generación usando las funciones de mutación y cruces\n",
    "- Criterios de parada\n",
    "- Registro de análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección por torneo\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tournament_selection_best5(population):\n",
    "    \"\"\"\n",
    "    Selecciona los 4 mejores individuos de la población mediante un torneo de tamaño fijo.\n",
    "\n",
    "    Args:\n",
    "        population: La lista de individuos con sus puntuaciones de fitness.\n",
    "\n",
    "    Returns:\n",
    "        Lista de los 4 mejores individuos.\n",
    "    \"\"\"\n",
    "    winners = []\n",
    "    for _ in range(5):\n",
    "        candidates = random.sample(population, 2)\n",
    "        winner = max(candidates, key=lambda x: x['fitness'])\n",
    "        winners.append(winner)\n",
    "    return winners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALGORITMO EVOLUTIVO\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfitness_scores = evaluate_population(population, train_loader, val_loader, device)\\n\\n# Almacenamos- los individuos y sus puntuaciones en una lista de tuplas y ordenarlos\\npopulation_with_scores = list(zip(population, fitness_scores))\\npopulation_with_scores.sort(key=lambda x: x[1], reverse=True)  # Ordena de mayor a menor aptitud\\n\\n# Imprimimimos los resultados\\nfor i, (individual, score) in enumerate(population_with_scores):\\n    print(f\"Descendecia: {i} Individuo {individual}: Pérdida: {score}\")\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "fitness_scores = evaluate_population(population, train_loader, val_loader, device)\n",
    "\n",
    "# Almacenamos- los individuos y sus puntuaciones en una lista de tuplas y ordenarlos\n",
    "population_with_scores = list(zip(population, fitness_scores))\n",
    "population_with_scores.sort(key=lambda x: x[1], reverse=True)  # Ordena de mayor a menor aptitud\n",
    "\n",
    "# Imprimimimos los resultados\n",
    "for i, (individual, score) in enumerate(population_with_scores):\n",
    "    print(f\"Descendecia: {i} Individuo {individual}: Pérdida: {score}\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = []\n",
    "\n",
    "def genetic_algorithm(population, train_loader, val_loader, device, max_desc, epochs):\n",
    "    \"\"\" \n",
    "    Algoritmo genetico para evolucionar una poblacion de arquitecturas hacia la mejor puntuacion dado un dataset dado\n",
    "    \"\"\"\n",
    "    desc = 0\n",
    "    while desc < max_desc:\n",
    "        print(\"***************************************\")\n",
    "        print()\n",
    "        print(f\"Generation: {desc}\")\n",
    "        print()\n",
    "        print(\"***************************************\")\n",
    "        puntuaciones_aptitud = evaluate_population(population, train_loader, val_loader, device, epochs)\n",
    "\n",
    "        # Ordenamos individuos\n",
    "        # Añadimos las puntuaciones directamente a cada diccionario de la población\n",
    "        for i, puntuacion in enumerate(puntuaciones_aptitud):\n",
    "            population[i]['fitness'] = puntuacion\n",
    "\n",
    "        population.sort(key=lambda x: x['fitness'], reverse=True)\n",
    "\n",
    "        # Preservamos el mejor individuo (elitismo)\n",
    "        mejor_individuo = population[0]\n",
    "        top_models.append(mejor_individuo)\n",
    "        # Seleccionamos los 4 mejores (excluyendo el mejor) para el torneo\n",
    "        winners = tournament_selection_best5(population[1:])\n",
    "        print(f\"Los 4 mejores son: \\n {winners}\")\n",
    "\n",
    "        # Realizamos cruce y mutación con descendientes que reemplazan a la población restante\n",
    "        for i in range(1, 5):\n",
    "            descendency = mutate_individual(crossover(winners[i-1], winners[i]))\n",
    "            population[-i] = descendency   # Eliminamos las peores arquitecturas\n",
    "        \n",
    "        # Nos aseguramos que mantenemos el mejor de todos\n",
    "        population[0] = mejor_individuo\n",
    "        desc += 1\n",
    "\n",
    "    return population\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTADOS DE LAS ARQUITECTURAS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************\n",
      "\n",
      "Generation: 0\n",
      "\n",
      "***************************************\n",
      "{'num_conv_layers': 1, 'filters': [16], 'kernel_sizes': [3], 'learning_rate': 0.0001, 'fully_connected': 2, 'dropout': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 391/391 [00:14<00:00, 26.14batch/s, training_loss=3.159]\n",
      "Epoch 2/15: 100%|██████████| 391/391 [00:13<00:00, 28.04batch/s, training_loss=3.106]\n",
      "Epoch 3/15: 100%|██████████| 391/391 [00:14<00:00, 27.58batch/s, training_loss=2.684]\n",
      "Epoch 4/15: 100%|██████████| 391/391 [00:13<00:00, 27.97batch/s, training_loss=2.112]\n",
      "Epoch 5/15: 100%|██████████| 391/391 [00:14<00:00, 27.55batch/s, training_loss=2.361]\n",
      "Epoch 6/15: 100%|██████████| 391/391 [00:13<00:00, 28.03batch/s, training_loss=2.365]\n",
      "Epoch 7/15: 100%|██████████| 391/391 [00:14<00:00, 27.52batch/s, training_loss=1.871]\n",
      "Epoch 8/15: 100%|██████████| 391/391 [00:14<00:00, 27.83batch/s, training_loss=2.339]\n",
      "Epoch 9/15: 100%|██████████| 391/391 [00:13<00:00, 28.05batch/s, training_loss=1.966]\n",
      "Epoch 10/15: 100%|██████████| 391/391 [00:14<00:00, 27.42batch/s, training_loss=2.167]\n",
      "Epoch 11/15: 100%|██████████| 391/391 [00:14<00:00, 27.15batch/s, training_loss=2.104]\n",
      "Epoch 12/15: 100%|██████████| 391/391 [00:14<00:00, 26.63batch/s, training_loss=1.691]\n",
      "Epoch 13/15: 100%|██████████| 391/391 [00:14<00:00, 27.08batch/s, training_loss=2.275]\n",
      "Epoch 14/15: 100%|██████████| 391/391 [00:14<00:00, 26.45batch/s, training_loss=1.549]\n",
      "Epoch 15/15: 100%|██████████| 391/391 [00:14<00:00, 27.12batch/s, training_loss=1.556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_conv_layers': 1, 'filters': [32], 'kernel_sizes': [1], 'learning_rate': 1e-06, 'fully_connected': 2, 'dropout': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 391/391 [00:31<00:00, 12.23batch/s, training_loss=4.005]\n",
      "Epoch 2/15: 100%|██████████| 391/391 [00:31<00:00, 12.33batch/s, training_loss=3.795]\n",
      "Epoch 3/15: 100%|██████████| 391/391 [00:31<00:00, 12.25batch/s, training_loss=3.724]\n",
      "Epoch 4/15: 100%|██████████| 391/391 [00:31<00:00, 12.24batch/s, training_loss=3.716]\n",
      "Epoch 5/15: 100%|██████████| 391/391 [00:32<00:00, 12.21batch/s, training_loss=3.401]\n",
      "Epoch 6/15: 100%|██████████| 391/391 [00:31<00:00, 12.26batch/s, training_loss=3.487]\n",
      "Epoch 7/15: 100%|██████████| 391/391 [00:32<00:00, 12.21batch/s, training_loss=3.488]\n",
      "Epoch 8/15: 100%|██████████| 391/391 [00:31<00:00, 12.24batch/s, training_loss=3.580]\n",
      "Epoch 9/15: 100%|██████████| 391/391 [00:32<00:00, 12.21batch/s, training_loss=3.399]\n",
      "Epoch 10/15: 100%|██████████| 391/391 [00:32<00:00, 12.15batch/s, training_loss=3.412]\n",
      "Epoch 11/15: 100%|██████████| 391/391 [00:32<00:00, 12.18batch/s, training_loss=3.128]\n",
      "Epoch 12/15: 100%|██████████| 391/391 [00:31<00:00, 12.25batch/s, training_loss=3.200]\n",
      "Epoch 13/15: 100%|██████████| 391/391 [00:31<00:00, 12.26batch/s, training_loss=3.168]\n",
      "Epoch 14/15: 100%|██████████| 391/391 [00:32<00:00, 12.14batch/s, training_loss=3.278]\n",
      "Epoch 15/15: 100%|██████████| 391/391 [00:32<00:00, 12.20batch/s, training_loss=3.584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_conv_layers': 2, 'filters': [8, 16], 'kernel_sizes': [7, 1], 'learning_rate': 0.01, 'fully_connected': 2, 'dropout': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 391/391 [00:14<00:00, 27.48batch/s, training_loss=4.868]   \n",
      "Epoch 2/15: 100%|██████████| 391/391 [00:14<00:00, 27.35batch/s, training_loss=4.638]\n",
      "Epoch 3/15: 100%|██████████| 391/391 [00:15<00:00, 25.15batch/s, training_loss=4.401]\n",
      "Epoch 4/15: 100%|██████████| 391/391 [00:14<00:00, 26.46batch/s, training_loss=4.754]\n",
      "Epoch 5/15: 100%|██████████| 391/391 [00:14<00:00, 26.83batch/s, training_loss=4.781]\n",
      "Epoch 6/15: 100%|██████████| 391/391 [00:14<00:00, 26.66batch/s, training_loss=4.358]\n",
      "Epoch 7/15: 100%|██████████| 391/391 [00:16<00:00, 24.42batch/s, training_loss=4.317]\n",
      "Epoch 8/15: 100%|██████████| 391/391 [00:15<00:00, 25.74batch/s, training_loss=4.282]\n",
      "Epoch 9/15: 100%|██████████| 391/391 [00:14<00:00, 27.62batch/s, training_loss=4.276]\n",
      "Epoch 10/15: 100%|██████████| 391/391 [00:14<00:00, 26.75batch/s, training_loss=3.821]\n",
      "Epoch 11/15: 100%|██████████| 391/391 [00:14<00:00, 27.65batch/s, training_loss=3.961]\n",
      "Epoch 12/15: 100%|██████████| 391/391 [00:14<00:00, 27.53batch/s, training_loss=3.741]\n",
      "Epoch 13/15: 100%|██████████| 391/391 [00:14<00:00, 27.87batch/s, training_loss=4.070]\n",
      "Epoch 14/15: 100%|██████████| 391/391 [00:14<00:00, 27.32batch/s, training_loss=4.196]\n",
      "Epoch 15/15: 100%|██████████| 391/391 [00:14<00:00, 27.57batch/s, training_loss=4.449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_conv_layers': 2, 'filters': [64, 128], 'kernel_sizes': [5, 3], 'learning_rate': 0.01, 'fully_connected': 1, 'dropout': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 391/391 [07:44<00:00,  1.19s/batch, training_loss=4.828]   \n",
      "Epoch 2/15: 100%|██████████| 391/391 [07:38<00:00,  1.17s/batch, training_loss=4.735]\n",
      "Epoch 3/15: 100%|██████████| 391/391 [07:39<00:00,  1.17s/batch, training_loss=4.534]\n",
      "Epoch 4/15: 100%|██████████| 391/391 [07:39<00:00,  1.17s/batch, training_loss=4.533]\n",
      "Epoch 5/15: 100%|██████████| 391/391 [07:38<00:00,  1.17s/batch, training_loss=4.626]\n",
      "Epoch 6/15: 100%|██████████| 391/391 [07:38<00:00,  1.17s/batch, training_loss=4.549]\n",
      "Epoch 7/15: 100%|██████████| 391/391 [07:39<00:00,  1.17s/batch, training_loss=4.655]\n",
      "Epoch 8/15: 100%|██████████| 391/391 [07:39<00:00,  1.18s/batch, training_loss=5.356]\n",
      "Epoch 9/15: 100%|██████████| 391/391 [07:38<00:00,  1.17s/batch, training_loss=4.719]\n",
      "Epoch 10/15: 100%|██████████| 391/391 [07:38<00:00,  1.17s/batch, training_loss=5.153]\n",
      "Epoch 11/15: 100%|██████████| 391/391 [07:38<00:00,  1.17s/batch, training_loss=4.588]\n",
      "Epoch 12/15: 100%|██████████| 391/391 [07:38<00:00,  1.17s/batch, training_loss=4.618]\n",
      "Epoch 13/15: 100%|██████████| 391/391 [07:38<00:00,  1.17s/batch, training_loss=4.384]\n",
      "Epoch 14/15: 100%|██████████| 391/391 [07:38<00:00,  1.17s/batch, training_loss=4.410]\n",
      "Epoch 15/15: 100%|██████████| 391/391 [07:38<00:00,  1.17s/batch, training_loss=4.564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_conv_layers': 1, 'filters': [64], 'kernel_sizes': [3], 'learning_rate': 0.0001, 'fully_connected': 0, 'dropout': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 391/391 [00:12<00:00, 32.15batch/s, training_loss=3.083]\n",
      "Epoch 2/15: 100%|██████████| 391/391 [00:14<00:00, 27.72batch/s, training_loss=2.746]\n",
      "Epoch 3/15: 100%|██████████| 391/391 [00:14<00:00, 27.64batch/s, training_loss=2.385]\n",
      "Epoch 4/15: 100%|██████████| 391/391 [00:14<00:00, 27.14batch/s, training_loss=2.518]\n",
      "Epoch 5/15: 100%|██████████| 391/391 [00:14<00:00, 27.30batch/s, training_loss=1.901]\n",
      "Epoch 6/15: 100%|██████████| 391/391 [00:13<00:00, 28.02batch/s, training_loss=2.145]\n",
      "Epoch 7/15: 100%|██████████| 391/391 [00:13<00:00, 28.05batch/s, training_loss=2.139]\n",
      "Epoch 8/15: 100%|██████████| 391/391 [00:14<00:00, 27.68batch/s, training_loss=1.725]\n",
      "Epoch 9/15: 100%|██████████| 391/391 [00:14<00:00, 27.87batch/s, training_loss=1.978]\n",
      "Epoch 10/15: 100%|██████████| 391/391 [00:13<00:00, 27.98batch/s, training_loss=1.430]\n",
      "Epoch 11/15: 100%|██████████| 391/391 [00:14<00:00, 27.62batch/s, training_loss=1.596]\n",
      "Epoch 12/15: 100%|██████████| 391/391 [00:13<00:00, 28.41batch/s, training_loss=1.248]\n",
      "Epoch 13/15: 100%|██████████| 391/391 [00:13<00:00, 27.93batch/s, training_loss=1.143]\n",
      "Epoch 14/15: 100%|██████████| 391/391 [00:13<00:00, 27.98batch/s, training_loss=1.047]\n",
      "Epoch 15/15: 100%|██████████| 391/391 [00:14<00:00, 27.71batch/s, training_loss=1.120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_conv_layers': 1, 'filters': [16], 'kernel_sizes': [5], 'learning_rate': 0.0001, 'fully_connected': 1, 'dropout': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 391/391 [00:14<00:00, 27.05batch/s, training_loss=3.170]\n",
      "Epoch 2/15: 100%|██████████| 391/391 [00:14<00:00, 27.41batch/s, training_loss=3.008]\n",
      "Epoch 3/15: 100%|██████████| 391/391 [00:14<00:00, 27.75batch/s, training_loss=2.386]\n",
      "Epoch 4/15: 100%|██████████| 391/391 [00:14<00:00, 27.81batch/s, training_loss=2.388]\n",
      "Epoch 5/15: 100%|██████████| 391/391 [00:14<00:00, 27.47batch/s, training_loss=3.066]\n",
      "Epoch 6/15: 100%|██████████| 391/391 [00:14<00:00, 27.60batch/s, training_loss=2.552]\n",
      "Epoch 7/15: 100%|██████████| 391/391 [00:14<00:00, 27.46batch/s, training_loss=2.222]\n",
      "Epoch 8/15: 100%|██████████| 391/391 [00:14<00:00, 27.42batch/s, training_loss=2.155]\n",
      "Epoch 9/15: 100%|██████████| 391/391 [00:14<00:00, 27.74batch/s, training_loss=2.043]\n",
      "Epoch 10/15: 100%|██████████| 391/391 [00:14<00:00, 27.78batch/s, training_loss=2.140]\n",
      "Epoch 11/15: 100%|██████████| 391/391 [00:14<00:00, 27.73batch/s, training_loss=1.555]\n",
      "Epoch 12/15: 100%|██████████| 391/391 [00:14<00:00, 27.47batch/s, training_loss=1.420]\n",
      "Epoch 13/15: 100%|██████████| 391/391 [00:14<00:00, 27.50batch/s, training_loss=1.646]\n",
      "Epoch 14/15: 100%|██████████| 391/391 [00:14<00:00, 27.42batch/s, training_loss=1.422]\n",
      "Epoch 15/15: 100%|██████████| 391/391 [00:14<00:00, 27.91batch/s, training_loss=1.750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_conv_layers': 2, 'filters': [128, 64], 'kernel_sizes': [11, 1], 'learning_rate': 0.01, 'fully_connected': 2, 'dropout': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 391/391 [00:43<00:00,  8.95batch/s, training_loss=34.497]   \n",
      "Epoch 2/15: 100%|██████████| 391/391 [00:43<00:00,  9.05batch/s, training_loss=19.466]\n",
      "Epoch 3/15: 100%|██████████| 391/391 [00:43<00:00,  9.04batch/s, training_loss=33.911]\n",
      "Epoch 4/15: 100%|██████████| 391/391 [00:43<00:00,  8.99batch/s, training_loss=13.354]\n",
      "Epoch 5/15: 100%|██████████| 391/391 [00:43<00:00,  8.98batch/s, training_loss=12.169]\n",
      "Epoch 6/15: 100%|██████████| 391/391 [00:43<00:00,  8.98batch/s, training_loss=19.360]\n",
      "Epoch 7/15: 100%|██████████| 391/391 [00:43<00:00,  8.98batch/s, training_loss=12.191]\n",
      "Epoch 8/15: 100%|██████████| 391/391 [00:43<00:00,  8.98batch/s, training_loss=15.898]\n",
      "Epoch 9/15: 100%|██████████| 391/391 [00:43<00:00,  8.99batch/s, training_loss=6.283] \n",
      "Epoch 10/15: 100%|██████████| 391/391 [00:43<00:00,  8.98batch/s, training_loss=5.227] \n",
      "Epoch 11/15: 100%|██████████| 391/391 [00:43<00:00,  8.97batch/s, training_loss=7.327] \n",
      "Epoch 12/15: 100%|██████████| 391/391 [00:43<00:00,  8.97batch/s, training_loss=4.888]\n",
      "Epoch 13/15: 100%|██████████| 391/391 [00:43<00:00,  8.97batch/s, training_loss=4.748]\n",
      "Epoch 14/15: 100%|██████████| 391/391 [00:43<00:00,  8.97batch/s, training_loss=4.698]\n",
      "Epoch 15/15: 100%|██████████| 391/391 [00:44<00:00,  8.85batch/s, training_loss=4.818] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_conv_layers': 1, 'filters': [16], 'kernel_sizes': [3], 'learning_rate': 1e-06, 'fully_connected': 2, 'dropout': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 391/391 [00:14<00:00, 26.64batch/s, training_loss=4.307]\n",
      "Epoch 2/15: 100%|██████████| 391/391 [00:14<00:00, 26.24batch/s, training_loss=4.175]\n",
      "Epoch 3/15: 100%|██████████| 391/391 [00:14<00:00, 26.86batch/s, training_loss=4.146]\n",
      "Epoch 4/15: 100%|██████████| 391/391 [00:14<00:00, 26.64batch/s, training_loss=3.976]\n",
      "Epoch 5/15: 100%|██████████| 391/391 [00:14<00:00, 26.68batch/s, training_loss=3.688]\n",
      "Epoch 6/15: 100%|██████████| 391/391 [00:14<00:00, 26.86batch/s, training_loss=3.701]\n",
      "Epoch 7/15: 100%|██████████| 391/391 [00:14<00:00, 26.43batch/s, training_loss=3.619]\n",
      "Epoch 8/15: 100%|██████████| 391/391 [00:14<00:00, 27.11batch/s, training_loss=3.583]\n",
      "Epoch 9/15: 100%|██████████| 391/391 [00:14<00:00, 26.86batch/s, training_loss=3.589]\n",
      "Epoch 10/15: 100%|██████████| 391/391 [00:14<00:00, 27.06batch/s, training_loss=3.742]\n",
      "Epoch 11/15: 100%|██████████| 391/391 [00:14<00:00, 26.72batch/s, training_loss=3.568]\n",
      "Epoch 12/15: 100%|██████████| 391/391 [00:14<00:00, 27.13batch/s, training_loss=3.591]\n",
      "Epoch 13/15: 100%|██████████| 391/391 [00:14<00:00, 26.93batch/s, training_loss=3.512]\n",
      "Epoch 14/15: 100%|██████████| 391/391 [00:14<00:00, 26.94batch/s, training_loss=3.654]\n",
      "Epoch 15/15: 100%|██████████| 391/391 [00:14<00:00, 27.09batch/s, training_loss=3.520]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_conv_layers': 2, 'filters': [64, 32], 'kernel_sizes': [5, 3], 'learning_rate': 0.01, 'fully_connected': 0, 'dropout': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 391/391 [00:13<00:00, 27.94batch/s, training_loss=3.696]\n",
      "Epoch 2/15: 100%|██████████| 391/391 [00:14<00:00, 27.55batch/s, training_loss=2.972]\n",
      "Epoch 3/15: 100%|██████████| 391/391 [00:14<00:00, 27.58batch/s, training_loss=3.081]\n",
      "Epoch 4/15: 100%|██████████| 391/391 [00:14<00:00, 27.71batch/s, training_loss=2.636]\n",
      "Epoch 5/15: 100%|██████████| 391/391 [00:14<00:00, 27.36batch/s, training_loss=2.710]\n",
      "Epoch 6/15: 100%|██████████| 391/391 [00:14<00:00, 27.83batch/s, training_loss=2.387]\n",
      "Epoch 7/15: 100%|██████████| 391/391 [00:14<00:00, 27.44batch/s, training_loss=2.550]\n",
      "Epoch 8/15: 100%|██████████| 391/391 [00:14<00:00, 27.81batch/s, training_loss=2.076]\n",
      "Epoch 9/15: 100%|██████████| 391/391 [00:14<00:00, 27.50batch/s, training_loss=2.253]\n",
      "Epoch 10/15: 100%|██████████| 391/391 [00:14<00:00, 27.64batch/s, training_loss=1.864]\n",
      "Epoch 11/15: 100%|██████████| 391/391 [00:14<00:00, 27.81batch/s, training_loss=1.694]\n",
      "Epoch 12/15: 100%|██████████| 391/391 [00:14<00:00, 27.46batch/s, training_loss=1.788]\n",
      "Epoch 13/15: 100%|██████████| 391/391 [00:14<00:00, 27.72batch/s, training_loss=1.585]\n",
      "Epoch 14/15: 100%|██████████| 391/391 [00:14<00:00, 27.56batch/s, training_loss=1.149]\n",
      "Epoch 15/15: 100%|██████████| 391/391 [00:14<00:00, 27.86batch/s, training_loss=1.390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_conv_layers': 3, 'filters': [32, 64, 32], 'kernel_sizes': [1, 1, 1], 'learning_rate': 0.0001, 'fully_connected': 2, 'dropout': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 391/391 [00:47<00:00,  8.25batch/s, training_loss=3.363]\n",
      "Epoch 2/15: 100%|██████████| 391/391 [00:47<00:00,  8.31batch/s, training_loss=3.279]\n",
      "Epoch 3/15: 100%|██████████| 391/391 [00:47<00:00,  8.27batch/s, training_loss=3.353]\n",
      "Epoch 4/15: 100%|██████████| 391/391 [00:47<00:00,  8.27batch/s, training_loss=2.918]\n",
      "Epoch 5/15: 100%|██████████| 391/391 [00:47<00:00,  8.27batch/s, training_loss=2.813]\n",
      "Epoch 6/15: 100%|██████████| 391/391 [00:47<00:00,  8.24batch/s, training_loss=3.004]\n",
      "Epoch 7/15: 100%|██████████| 391/391 [00:47<00:00,  8.25batch/s, training_loss=2.891]\n",
      "Epoch 8/15: 100%|██████████| 391/391 [00:47<00:00,  8.23batch/s, training_loss=2.530]\n",
      "Epoch 9/15: 100%|██████████| 391/391 [00:47<00:00,  8.24batch/s, training_loss=2.853]\n",
      "Epoch 10/15: 100%|██████████| 391/391 [00:47<00:00,  8.25batch/s, training_loss=2.370]\n",
      "Epoch 11/15: 100%|██████████| 391/391 [00:47<00:00,  8.25batch/s, training_loss=2.343]\n",
      "Epoch 12/15: 100%|██████████| 391/391 [00:47<00:00,  8.24batch/s, training_loss=2.760]\n",
      "Epoch 13/15: 100%|██████████| 391/391 [00:47<00:00,  8.26batch/s, training_loss=2.249]\n",
      "Epoch 14/15: 100%|██████████| 391/391 [00:47<00:00,  8.26batch/s, training_loss=2.215]\n",
      "Epoch 15/15: 100%|██████████| 391/391 [00:47<00:00,  8.24batch/s, training_loss=1.781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 4 mejores son: \n",
      " [{'num_conv_layers': 1, 'filters': [32], 'kernel_sizes': [1], 'learning_rate': 1e-06, 'fully_connected': 2, 'dropout': 1, 'fitness': 0.2286}, {'num_conv_layers': 2, 'filters': [64, 128], 'kernel_sizes': [5, 3], 'learning_rate': 0.01, 'fully_connected': 1, 'dropout': 2, 'fitness': 0.0229}]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m population_sol \u001b[38;5;241m=\u001b[39m \u001b[43mgenetic_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 32\u001b[0m, in \u001b[0;36mgenetic_algorithm\u001b[1;34m(population, train_loader, val_loader, device, max_desc, epochs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Realizamos cruce y mutación con descendientes que reemplazan a la población restante\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m---> 32\u001b[0m     descendency \u001b[38;5;241m=\u001b[39m mutate_individual(crossover(winners[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[43mwinners\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[0;32m     33\u001b[0m     population[\u001b[38;5;241m-\u001b[39mi] \u001b[38;5;241m=\u001b[39m descendency   \u001b[38;5;66;03m# Eliminamos las peores arquitecturas\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Nos aseguramos que mantenemos el mejor de todos\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "population_sol = genetic_algorithm(population, train_loader, val_loader, device, max_desc, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for individual in top_models:\n",
    "    print(f\"Individuo {individual} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = build_cnn_from_individual(top_models[len(top_models)-1])\n",
    "model = model.to(device)\n",
    "summary(model, (num_channels, px_h, px_w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
